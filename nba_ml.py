# -*- coding: utf-8 -*-
"""NBA ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Imh1ZGk-lhhAWOa1ogWOc4eLbrveS_n
"""

# Download the API for the scraper so we can gather our information.
pip install basketball-reference-scraper==v1.0.1

from basketball_reference_scraper.teams import get_roster, get_team_stats, get_opp_stats, get_roster_stats, get_team_misc
import pandas as pd
import numpy as np

# We want to predict the position of a player by looking specifically at their in-game stats.
# These stats would be like 3P%, FG%, etc.
# The data ranges from 2014 to 2019.

years = list(range(2014, 2020))
print(years)

link = 'https://en.wikipedia.org/wiki/Wikipedia:WikiProject_National_Basketball_Association/National_Basketball_Association_team_abbreviations'

team_table = pd.read_html(link, header = 0)
team_table = team_table[0]

team_table

# Now we can just grab the team acronyms by making the column a series.
teams = list(team_table['Abbreviation/Acronym'])

print(teams)

# Now we want to match each year with a team so that we can input it into the scraper.

matches = [ [a, b] for a in teams 
           for b in years if a != b]

print(matches)

# Now that we have the data that we'd like to input, we can now use the API to load in our data into a Pandas dataframe.

def get_info(team, year):
  try: 
    return get_roster_stats(team, year)
  except:
    return None

data = [get_info(i[0], i[1]) for i in matches]

data

data = pd.concat(data)

# Awesome, now let's verify that we actually have all the teams. 
pd.value_counts(data['TEAM'])

# There is only a total of 28 teams, let's see what the differences are.

set(teams).difference(data['TEAM'])

# The documentation has BRK instead of BKN, and PHO instead of PHX

teams = ['BRK' if team == 'BKN' else team for team in teams]
teams = ['PHO' if team == 'PHX' else team for team in teams]

print(teams)

# Now we can just rerun the above, and see if we have 30 teams.

matches = [ [a, b] for a in teams 
           for b in years if a != b]

data = [get_info(i[0], i[1]) for i in matches]
data = pd.concat(data)

# Awesome, we have all the teams now.
len(data['TEAM'].unique())

data.to_csv("data.csv", index = False)

# We can now extract the features that we can, as well as the value that we cant to predict (position).

data

# Unfortunately, with how scikit-learn works, any values with NaN (none types) cannot be used in the algorithm.
# Therefore, we'd have to drop them before we enter. 
data = data.dropna()

# It looks here like we lose a good amount of players. 
# Fortunately enough, most players with NaN are usually those that don't play enough minutes, or played at all.

data

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV

# We'll be using a Random Forest due to the high number of features present in our data. 
# Random forests have been proven to be one of the better models when dealing with multiple features.

# First, lets split the data into two sets: training and testing.
# A common ratio is 80% test, 20% train, so let's roll with that.

position = data['POS']
features = data[ ['MP', 'FG%', '3P%', '2P%', 'eFG%', 'FT%', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']   ]


train_features, test_features, train_class, test_class = train_test_split(features, position, test_size = 0.20)

# Now let's run an unoptimized random forest classifier.
rf = RandomForestClassifier()

rf.fit(train_features, train_class)
rf_result = rf.predict(test_features)
error_rf = accuracy_score(test_class, rf_result)

print(error_rf)

# We can actually optimize an RF to see if we can grab better results with the classifier.

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_opt = RandomizedSearchCV(estimator = rf, 
                               param_distributions = random_grid, 
                               n_iter = 100, cv = 3, verbose=2, 
                               random_state=42, n_jobs = -1)

rf_opt.fit(train_features, train_class)

params_rf = rf_opt.best_params_

opt_rf = RandomForestClassifier(**params_rf)
opt_rf.fit(train_features, train_class)
opt_rf_result = opt_rf.predict(test_features)

opt_rf_error = accuracy_score(test_class, opt_rf_result)
print(opt_rf_error)


# The optimizer did nothing. Compare the results to a basic LogisticRegression

from sklearn.linear_model import LogisticRegression

lg = LogisticRegression(max_iter = 100000)
lg.fit(train_features, train_class)
lg_results = lg.predict(test_features)

lg_error = accuracy_score(lg_results, test_class)
print(lg_error)

# With the error AFTER optimization for random forest having essentially 0 effect on the accuracy, it's honestly not worth
# it to optimize the logistic regression. It is computationally expensive in doing so. One thing to note is that the errors
# are very similar. Both have around a 66% success rate.

# Let's plot the error rates for easier viewing.
import seaborn as sns
import matplotlib.pyplot as plt

errors = {
    'Random Forest': error_rf,
    'Logistic Regression': lg_error,
    'Opt. Random Forest': opt_rf_error
}

errors = pd.DataFrame(errors, index = [0])

plt.figure(figsize = [10,6])
sns.barplot(data = errors)
plt.title('Accuracy of Classification Algorithms for Years 2014 to 2019')
plt.savefig('errors_14-19')

# With the error AFTER optimization for random forest having essentially 0 effect on the accuracy, it's honestly not worth
# it to optimize the logistic regression. It is computationally expensive in doing so. One thing to note is that the errors
# are very similar. Both have around a 66% success rate.

# Now that we're through with that, let get another era. Let's choose 2003-2008, another 5 year gap.
years_2 = list(range(2004,2010))
print(years_2)

# We essentially can just do the same thing we did as above. Just have to make sure that the team codes line-up with the API.

# So now, we just replace the team codes that we used similar to how we did above.
# Things to note, Bobcats are in place of the Hornets, and the Hornets are in place of the Pelicans. 
# Also, the Nets are in New Jersey instead of Brooklyn.

teams = ['NJN' if team == 'BKN' else team for team in teams]
teams = ['PHO' if team == 'PHX' else team for team in teams]
teams = ['CHA' if team == 'CHO' else team for team in teams]

# We also have to add in the Supersonics, as this was around the time they fully transitioned into the OKC Thunder.

teams.append('SEA')

# When we run the algorithm to scrape BBALL-REF, we should have 31 teams. 


matches_2 = [ [a, b] for a in teams 
           for b in years_2 if a != b]

print(matches_2)

data2 = [get_info(i[0], i[1]) for i in matches_2]

data2 = pd.concat(data2)

data2

data2 = data2.dropna()

data2

# We have about 2400 rows to play with give or take that did NOT have NaN values. 
# Now for the fun part, re-run the same algorithms and see how the results differ for the two eras. 
# Our guess is that this era would have a higher accuracy rate.

position = data2['POS']
features = data2[ ['MP', 'FG%', '3P%', '2P%', 'eFG%', 'FT%', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']   ]


train_features, test_features, train_class, test_class = train_test_split(features, position, test_size = 0.20)

rf = RandomForestClassifier()

rf.fit(train_features, train_class)
rf_result = rf.predict(test_features)
error_rf_2 = accuracy_score(test_class, rf_result)

print(error_rf_2)

# Wow, the error is more or less the same for the two different eras. Let's attempt to optimize it, and see if there are any changes.
# After, we can go back to a base logistic regression.

rf_opt = RandomizedSearchCV(estimator = rf, 
                               param_distributions = random_grid, 
                               n_iter = 100, cv = 3, verbose=2, 
                               random_state=42, n_jobs = -1)

rf_opt.fit(train_features, train_class)

params_rf = rf_opt.best_params_

opt_rf = RandomForestClassifier(**params_rf)
opt_rf.fit(train_features, train_class)
opt_rf_result = opt_rf.predict(test_features)

opt_rf_error_2 = accuracy_score(test_class, opt_rf_result)
print(opt_rf_error_2)

# Again, go back to logistic regression.

lg = LogisticRegression(max_iter = 100000)
lg.fit(train_features, train_class)
lg_results = lg.predict(test_features)

lg_error_2 = accuracy_score(lg_results, test_class)
print(lg_error_2)

errors_2 = {
    'Random Forest': error_rf_2,
    'Logistic Regression': lg_error_2,
    'Opt. Random Forest': opt_rf_error_2
}

errors_2 = pd.DataFrame(errors_2, index = [0])

plt.figure(figsize = [10,6])
sns.barplot(data = errors_2)
plt.title('Accuracy of Classification Algorithms for Years 2004-2009')
plt.savefig('errors_04-09')

# Compare the highest performing algorithms for both.

compare = {
    '14 to 19': error_rf, 
    '04 to 09': lg_error_2
}

compare = pd.DataFrame(compare, index = [0])

sns.barplot(data = compare)
plt.xlabel('')
plt.ylabel('')
plt.title('Error Comparisons of different Eras')
plt.savefig('compare')

